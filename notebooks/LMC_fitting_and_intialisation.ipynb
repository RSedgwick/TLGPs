{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gpflow.base import _cast_to_dtype\n",
    "from models.test_functions import TestFunLinearCorrelation\n",
    "from gpflow.utilities import to_default_float, print_summary\n",
    "from gpflow.config import default_float\n",
    "# from gpflow.ci_utils import ci_niter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from models.utils import build_models, train_models, get_final_models_dict, get_gridpoints, get_nlpd, get_abs_error\n",
    "from models.lvmogp import LVMOGP\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# LMC fitting and initialisation\n",
    "\n",
    "This notebook is to demonstrate two things:\n",
    "1. The correct initalisation of the LMC needs W to be set randomly and kappa initialise close to zero for it to work.\n",
    "If W is not initalised randomly the model seems to always make W rank 1 which is not correct.\n",
    "2. The LMC can learn the new functions with only 2 points on the functions when there isn't any noise, but when the\n",
    "noise is larger, the LMC can be really wrong but also really confident with 2 data points. It does however recover the\n",
    "correct function with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the very low noise test functions\n",
    "\n",
    "Here we use the TestFun class to create a test function with the dimensions, latent dims, number of functions and noise\n",
    "that we want. The test functions are independent draws of a GP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "set_seed = True\n",
    "set_opt_seed = False\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "same_points = False\n",
    "plot_figs = True\n",
    "\n",
    "train_inducing = True\n",
    "opt_seed = 3\n",
    "domain = [0, 2]\n",
    "latent_dims = 2\n",
    "latent_dims_lvmogp = 2\n",
    "observed_dims = 1\n",
    "n_fun = 6\n",
    "n_points = [30]*2 + [2]*4\n",
    "max_points = 60\n",
    "noise = 1e-6\n",
    "n_grid_points = 100\n",
    "continuous_dims = ['x1', 'x2'][:observed_dims]\n",
    "lengthscales_x = [0.3, 0.3]\n",
    "\n",
    "\n",
    "if type(n_points) is int:\n",
    "    n_points = [n_points]*n_fun\n",
    "\n",
    "# Create the Test functions\n",
    "if set_seed:\n",
    "    seed = seed #3 #2 #3 #5 #4 #1 #2\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "\n",
    "test_fun = TestFunLinearCorrelation(domain, seed, n_fun, observed_dims, latent_dims, max_points, noise, n_grid_points,\n",
    "                                    same_points, lengthscales_x)\n",
    "\n",
    "data_X, fun_nos, data_y = test_fun.create_data(n_points=n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if plot_figs:\n",
    "    test_fun.plot_data_seperate_plots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_new = np.linspace(domain[0], domain[1], 100).reshape(100, 1)\n",
    "ys_new = []\n",
    "for fun in test_fun.functions:\n",
    "    y_new, _ = fun.predict_y(x_new)\n",
    "    ys_new.append(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "## Part 1: very low noise and no initalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the LMC\n",
    "\n",
    "With no initalisation of W or kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lengthscales_X = [0.1, 0.1][:len(continuous_dims)]\n",
    "lengthscales = lengthscales_X\n",
    "X_lmc = np.hstack([data_X, fun_nos])\n",
    "y = data_y\n",
    "k = gpflow.kernels.RBF(lengthscales=lengthscales, active_dims=range(observed_dims))\n",
    "coreg_k = gpflow.kernels.Coregion(output_dim=n_fun, rank=latent_dims ,active_dims=[observed_dims])\n",
    "cov = k * coreg_k\n",
    "lmc = gpflow.models.GPR(data=(tf.convert_to_tensor(X_lmc, dtype=default_float()),\n",
    "                                            tf.convert_to_tensor(y, dtype=default_float())), kernel=cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "maxiter = 1000\n",
    "\n",
    "lml = []\n",
    "def step_callback(step, variables, values):\n",
    "    lml.append(lmc.log_marginal_likelihood().numpy())\n",
    "lmc_opt = gpflow.optimizers.Scipy().minimize(\n",
    "    lmc.training_loss, lmc.trainable_variables, options=dict(maxiter=maxiter), method=\"L-BFGS-B\",\n",
    "    step_callback=step_callback)\n",
    "\n",
    "plt.plot(np.arange(len(lml)), np.array(lml))\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('step number')\n",
    "plt.ylabel('log marginal likelihood')\n",
    "plt.title('training curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fun_nos_new = np.hstack([[fun_no]*len(x_new) for fun_no in np.unique(fun_nos)])\n",
    "test = np.vstack([x_new]*n_fun)\n",
    "x_new_lmc = np.hstack([np.vstack([x_new]*n_fun), fun_nos_new.reshape(len(fun_nos_new), 1)])\n",
    "lmc_mu, lmc_sig2 = lmc.predict_y(x_new_lmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if plot_figs:\n",
    "    fig, axs = plt.subplots(1, ncols=n_fun, figsize=(3*n_fun, 3))\n",
    "    ax = axs.flatten()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] * 100\n",
    "    for i, fun_no in enumerate(np.unique(fun_nos)):\n",
    "        idx = np.where(fun_nos_new ==fun_no)[0]\n",
    "        ax[i].plot(x_new, lmc_mu.numpy()[idx], color=colors[i])\n",
    "        ax[i].scatter(test_fun.xs[i][:n_points[i]], test_fun.ys[i][:n_points[i]], color='k')\n",
    "        ax[i].plot(x_new, ys_new[i], linestyle=':', color='k')\n",
    "        ax[i].fill_between(x_new.flatten(), lmc_mu.numpy()[idx].flatten() + 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()),\n",
    "                           lmc_mu.numpy()[idx].flatten() - 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()), alpha=0.2, color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The rank of W ends up being 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lmc = lmc.kernel.kernels[1].W.numpy()\n",
    "print('rank of W:', np.linalg.matrix_rank(W_lmc))\n",
    "# B2 = np.dot(W_lmc, W_lmc.T)\n",
    "# print(np.linalg.matrix_rank(B2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('lmc log marginal likelihood',lmc.log_marginal_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print_summary(lmc)\n",
    "print(lmc.kernel.kernels[1].kappa.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "## Part 2: very low noise and initialising W randomly and kappa to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lengthscales_X = [0.1, 0.1][:len(continuous_dims)]\n",
    "lengthscales = lengthscales_X\n",
    "X_lmc = np.hstack([data_X, fun_nos])\n",
    "y = data_y\n",
    "k = gpflow.kernels.RBF(lengthscales=lengthscales, active_dims=range(observed_dims))\n",
    "coreg_k = gpflow.kernels.Coregion(output_dim=n_fun, rank=latent_dims ,active_dims=[observed_dims])\n",
    "cov = k * coreg_k\n",
    "lmc = gpflow.models.GPR(data=(tf.convert_to_tensor(X_lmc, dtype=default_float()),\n",
    "                                            tf.convert_to_tensor(y, dtype=default_float())), kernel=cov)\n",
    "lmc.kernel.kernels[1].W.assign(np.random.uniform(0.1, 1, [n_fun, latent_dims]))\n",
    "lmc.kernel.kernels[1].kappa.assign([1e-6]*n_fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "maxiter = 1000\n",
    "\n",
    "lml = []\n",
    "def step_callback(step, variables, values):\n",
    "    lml.append(lmc.log_marginal_likelihood().numpy())\n",
    "lmc_opt = gpflow.optimizers.Scipy().minimize(\n",
    "    lmc.training_loss, lmc.trainable_variables, options=dict(maxiter=maxiter), method=\"L-BFGS-B\",\n",
    "    step_callback=step_callback)\n",
    "\n",
    "plt.plot(np.arange(len(lml)), np.array(lml))\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('step number')\n",
    "plt.ylabel('log marginal likelihood')\n",
    "plt.title('training curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fun_nos_new = np.hstack([[fun_no]*len(x_new) for fun_no in np.unique(fun_nos)])\n",
    "test = np.vstack([x_new]*n_fun)\n",
    "x_new_lmc = np.hstack([np.vstack([x_new]*n_fun), fun_nos_new.reshape(len(fun_nos_new), 1)])\n",
    "lmc_mu, lmc_sig2 = lmc.predict_y(x_new_lmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "if plot_figs:\n",
    "    fig, axs = plt.subplots(1, ncols=n_fun, figsize=(3*n_fun, 3))\n",
    "    ax = axs.flatten()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] * 100\n",
    "    for i, fun_no in enumerate(np.unique(fun_nos)):\n",
    "        idx = np.where(fun_nos_new ==fun_no)[0]\n",
    "        ax[i].plot(x_new, lmc_mu.numpy()[idx], color=colors[i])\n",
    "        ax[i].scatter(test_fun.xs[i][:n_points[i]], test_fun.ys[i][:n_points[i]], color='k')\n",
    "        ax[i].plot(x_new, ys_new[i], linestyle=':', color='k')\n",
    "        ax[i].fill_between(x_new.flatten(), lmc_mu.numpy()[idx].flatten() + 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()),\n",
    "                           lmc_mu.numpy()[idx].flatten() - 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()), alpha=0.2, color=colors[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of W ends up being 2 which is correct as there are 2 latent functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lmc = lmc.kernel.kernels[1].W.numpy()\n",
    "print('rank of W:', np.linalg.matrix_rank(W_lmc))\n",
    "# B2 = np.dot(W_lmc, W_lmc.T)\n",
    "# print(np.linalg.matrix_rank(B2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The log marginal likeihood of this model is greater than that of the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('lmc log marginal likelihood',lmc.log_marginal_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print_summary(lmc)\n",
    "print(lmc.kernel.kernels[1].kappa.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "## Part 3: noisier data and initialising W randomly and kappa to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_fun.noise = 0.1\n",
    "\n",
    "test_fun.ys = []\n",
    "for i in range(test_fun.n_fun):\n",
    "    test_fun.ys.append(test_fun.function_with_noise(test_fun.functions[i], test_fun.xs[i], noise=test_fun.noise))\n",
    "\n",
    "data_X, fun_nos, data_y = test_fun.create_data(n_points=n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_X, fun_nos, data_y = test_fun.create_data(n_points=n_points)\n",
    "if plot_figs:\n",
    "    test_fun.plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the test functions and data on seperate plots so they can be seen better. Then plot the latent coordinates\n",
    "of the test functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_new = np.linspace(domain[0], domain[1], 100).reshape(100, 1)\n",
    "ys_new = []\n",
    "for fun in test_fun.functions:\n",
    "    y_new, _ = fun.predict_y(x_new)\n",
    "    ys_new.append(y_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lengthscales_X = [0.1, 0.1][:len(continuous_dims)]\n",
    "lengthscales = lengthscales_X\n",
    "X_lmc = np.hstack([data_X, fun_nos])\n",
    "y = data_y\n",
    "k = gpflow.kernels.RBF(lengthscales=lengthscales, active_dims=range(observed_dims))\n",
    "coreg_k = gpflow.kernels.Coregion(output_dim=n_fun, rank=latent_dims ,active_dims=[observed_dims])\n",
    "cov = k * coreg_k\n",
    "lmc = gpflow.models.GPR(data=(tf.convert_to_tensor(X_lmc, dtype=default_float()),\n",
    "                                            tf.convert_to_tensor(y, dtype=default_float())), kernel=cov)\n",
    "lmc.kernel.kernels[1].W.assign(np.random.uniform(0.1, 1, [n_fun, latent_dims]))\n",
    "lmc.kernel.kernels[1].kappa.assign([1e-6]*n_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "maxiter = 1000\n",
    "\n",
    "lml = []\n",
    "def step_callback(step, variables, values):\n",
    "    lml.append(lmc.log_marginal_likelihood().numpy())\n",
    "lmc_opt = gpflow.optimizers.Scipy().minimize(\n",
    "    lmc.training_loss, lmc.trainable_variables, options=dict(maxiter=maxiter), method=\"L-BFGS-B\",\n",
    "    step_callback=step_callback)\n",
    "\n",
    "plt.plot(np.arange(len(lml)), np.array(lml))\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('step number')\n",
    "plt.ylabel('log marginal likelihood')\n",
    "plt.title('training curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_new, fun_nos, x_new_lmc, x_new_lvmogp = get_gridpoints(domain, n_fun, {'lmc':lmc}, observed_dims, n_points=100)\n",
    "lmc_mu, lmc_sig2 = lmc.predict_y(x_new_lmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the model fits badly here, with some of the new functions being predicted very wrong but also very\n",
    "confidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if plot_figs:\n",
    "    fig, axs = plt.subplots(1, ncols=n_fun, figsize=(3*n_fun, 3))\n",
    "    ax = axs.flatten()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] * 100\n",
    "    for i, fun_no in enumerate(np.unique(fun_nos)):\n",
    "        idx = np.where(fun_nos_new ==fun_no)[0]\n",
    "        ax[i].plot(x_new, lmc_mu.numpy()[idx], color=colors[i])\n",
    "        ax[i].scatter(test_fun.xs[i][:n_points[i]], test_fun.ys[i][:n_points[i]], color='k')\n",
    "        ax[i].plot(x_new, ys_new[i], linestyle=':', color='k')\n",
    "        ax[i].fill_between(x_new.flatten(), lmc_mu.numpy()[idx].flatten() + 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()),\n",
    "                           lmc_mu.numpy()[idx].flatten() - 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()), alpha=0.2, color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of W ends up being 2 which is correct as there are 2 latent functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lmc = lmc.kernel.kernels[1].W.numpy()\n",
    "print('rank of W:', np.linalg.matrix_rank(W_lmc))\n",
    "# B2 = np.dot(W_lmc, W_lmc.T)\n",
    "# print(np.linalg.matrix_rank(B2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log marginal likeihood of this model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('lmc log marginal likelihood',lmc.log_marginal_likelihood())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print_summary(lmc)\n",
    "print(lmc.kernel.kernels[1].kappa.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Back to top](#Index:)\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "## Part 4: noisier data and initialising W randomly and kappa to zero but with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n_points = [30]*2 + [10]*4\n",
    "data_X, fun_nos, data_y = test_fun.create_data(n_points=n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the test functions and data on seperate plots so they can be seen better. Then plot the latent coordinates\n",
    "of the test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if plot_figs:\n",
    "    test_fun.plot_data_seperate_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_new = np.linspace(domain[0], domain[1], 100).reshape(100, 1)\n",
    "ys_new = []\n",
    "for fun in test_fun.functions:\n",
    "    y_new, _ = fun.predict_y(x_new)\n",
    "    ys_new.append(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lengthscales_X = [0.1, 0.1][:len(continuous_dims)]\n",
    "lengthscales = lengthscales_X\n",
    "X_lmc = np.hstack([data_X, fun_nos])\n",
    "y = data_y\n",
    "k = gpflow.kernels.RBF(lengthscales=lengthscales, active_dims=range(observed_dims))\n",
    "coreg_k = gpflow.kernels.Coregion(output_dim=n_fun, rank=latent_dims ,active_dims=[observed_dims])\n",
    "cov = k * coreg_k\n",
    "lmc = gpflow.models.GPR(data=(tf.convert_to_tensor(X_lmc, dtype=default_float()),\n",
    "                                            tf.convert_to_tensor(y, dtype=default_float())), kernel=cov)\n",
    "lmc.kernel.kernels[1].W.assign(np.random.uniform(0.1, 1, [n_fun, latent_dims]))\n",
    "lmc.kernel.kernels[1].kappa.assign([1e-6]*n_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "maxiter = 1000\n",
    "\n",
    "lml = []\n",
    "def step_callback(step, variables, values):\n",
    "    lml.append(lmc.log_marginal_likelihood().numpy())\n",
    "lmc_opt = gpflow.optimizers.Scipy().minimize(\n",
    "    lmc.training_loss, lmc.trainable_variables, options=dict(maxiter=maxiter), method=\"L-BFGS-B\",\n",
    "    step_callback=step_callback)\n",
    "\n",
    "plt.plot(np.arange(len(lml)), np.array(lml))\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('step number')\n",
    "plt.ylabel('log marginal likelihood')\n",
    "plt.title('training curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fun_nos_new = np.hstack([[fun_no]*len(x_new) for fun_no in np.unique(fun_nos)])\n",
    "test = np.vstack([x_new]*n_fun)\n",
    "x_new_lmc = np.hstack([np.vstack([x_new]*n_fun), fun_nos_new.reshape(len(fun_nos_new), 1)])\n",
    "lmc_mu, lmc_sig2 = lmc.predict_y(x_new_lmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see the model fits much better than when it only had 2 points per surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if plot_figs:\n",
    "    fig, axs = plt.subplots(1, ncols=n_fun, figsize=(3*n_fun, 3))\n",
    "    ax = axs.flatten()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] * 100\n",
    "    for i, fun_no in enumerate(np.unique(fun_nos)):\n",
    "        idx = np.where(fun_nos_new ==fun_no)[0]\n",
    "        ax[i].plot(x_new, lmc_mu.numpy()[idx], color=colors[i])\n",
    "        ax[i].scatter(test_fun.xs[i][:n_points[i]], test_fun.ys[i][:n_points[i]], color='k')\n",
    "        ax[i].plot(x_new, ys_new[i], linestyle=':', color='k')\n",
    "        ax[i].fill_between(x_new.flatten(), lmc_mu.numpy()[idx].flatten() + 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()),\n",
    "                           lmc_mu.numpy()[idx].flatten() - 2*np.sqrt(lmc_sig2.numpy()[idx].flatten()), alpha=0.2, color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of W ends up being 2 which is correct as there are 2 latent functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "W_lmc = lmc.kernel.kernels[1].W.numpy()\n",
    "print('rank of W:', np.linalg.matrix_rank(W_lmc))\n",
    "# B2 = np.dot(W_lmc, W_lmc.T)\n",
    "# print(np.linalg.matrix_rank(B2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log marginal likeihood of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('lmc log marginal likelihood',lmc.log_marginal_likelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print_summary(lmc)\n",
    "print(lmc.kernel.kernels[1].kappa.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}